{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-MlHW4r9ra_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Santander Customer Transaction Prediction\n",
        "We need 0.9 score on roc_auc_score\n",
        "So let's use a couple of potentially useful models - LR with/without PCA, lightgbm and a Linear Sequential Neural Network"
      ],
      "metadata": {
        "id": "-q5F2dmlraN8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iWNpFfOcPmt_"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "OgQh7n2kTZ_3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "pfMEoyzcT0HZ",
        "outputId": "4f068f0a-ff33-41d5-b711-d4fe456ae25a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   ID_code  target    var_0   var_1  ...  var_196  var_197  var_198  var_199\n",
              "0  train_0       0   8.9255 -6.7863  ...   7.8784   8.5635  12.7803  -1.0914\n",
              "1  train_1       0  11.5006 -4.1473  ...   8.1267   8.7889  18.3560   1.9518\n",
              "2  train_2       0   8.6093 -2.7457  ...  -6.5213   8.2675  14.7222   0.3965\n",
              "3  train_3       0  11.0604 -2.1518  ...  -2.9275  10.2922  17.9697  -8.9996\n",
              "4  train_4       0   9.8369 -1.4834  ...   3.9267   9.5031  17.9974  -8.8104\n",
              "\n",
              "[5 rows x 202 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d5dc105b-b475-4383-97ab-77c26ebcac65\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID_code</th>\n",
              "      <th>target</th>\n",
              "      <th>var_0</th>\n",
              "      <th>var_1</th>\n",
              "      <th>var_2</th>\n",
              "      <th>var_3</th>\n",
              "      <th>var_4</th>\n",
              "      <th>var_5</th>\n",
              "      <th>var_6</th>\n",
              "      <th>var_7</th>\n",
              "      <th>var_8</th>\n",
              "      <th>var_9</th>\n",
              "      <th>var_10</th>\n",
              "      <th>var_11</th>\n",
              "      <th>var_12</th>\n",
              "      <th>var_13</th>\n",
              "      <th>var_14</th>\n",
              "      <th>var_15</th>\n",
              "      <th>var_16</th>\n",
              "      <th>var_17</th>\n",
              "      <th>var_18</th>\n",
              "      <th>var_19</th>\n",
              "      <th>var_20</th>\n",
              "      <th>var_21</th>\n",
              "      <th>var_22</th>\n",
              "      <th>var_23</th>\n",
              "      <th>var_24</th>\n",
              "      <th>var_25</th>\n",
              "      <th>var_26</th>\n",
              "      <th>var_27</th>\n",
              "      <th>var_28</th>\n",
              "      <th>var_29</th>\n",
              "      <th>var_30</th>\n",
              "      <th>var_31</th>\n",
              "      <th>var_32</th>\n",
              "      <th>var_33</th>\n",
              "      <th>var_34</th>\n",
              "      <th>var_35</th>\n",
              "      <th>var_36</th>\n",
              "      <th>var_37</th>\n",
              "      <th>...</th>\n",
              "      <th>var_160</th>\n",
              "      <th>var_161</th>\n",
              "      <th>var_162</th>\n",
              "      <th>var_163</th>\n",
              "      <th>var_164</th>\n",
              "      <th>var_165</th>\n",
              "      <th>var_166</th>\n",
              "      <th>var_167</th>\n",
              "      <th>var_168</th>\n",
              "      <th>var_169</th>\n",
              "      <th>var_170</th>\n",
              "      <th>var_171</th>\n",
              "      <th>var_172</th>\n",
              "      <th>var_173</th>\n",
              "      <th>var_174</th>\n",
              "      <th>var_175</th>\n",
              "      <th>var_176</th>\n",
              "      <th>var_177</th>\n",
              "      <th>var_178</th>\n",
              "      <th>var_179</th>\n",
              "      <th>var_180</th>\n",
              "      <th>var_181</th>\n",
              "      <th>var_182</th>\n",
              "      <th>var_183</th>\n",
              "      <th>var_184</th>\n",
              "      <th>var_185</th>\n",
              "      <th>var_186</th>\n",
              "      <th>var_187</th>\n",
              "      <th>var_188</th>\n",
              "      <th>var_189</th>\n",
              "      <th>var_190</th>\n",
              "      <th>var_191</th>\n",
              "      <th>var_192</th>\n",
              "      <th>var_193</th>\n",
              "      <th>var_194</th>\n",
              "      <th>var_195</th>\n",
              "      <th>var_196</th>\n",
              "      <th>var_197</th>\n",
              "      <th>var_198</th>\n",
              "      <th>var_199</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train_0</td>\n",
              "      <td>0</td>\n",
              "      <td>8.9255</td>\n",
              "      <td>-6.7863</td>\n",
              "      <td>11.9081</td>\n",
              "      <td>5.0930</td>\n",
              "      <td>11.4607</td>\n",
              "      <td>-9.2834</td>\n",
              "      <td>5.1187</td>\n",
              "      <td>18.6266</td>\n",
              "      <td>-4.9200</td>\n",
              "      <td>5.7470</td>\n",
              "      <td>2.9252</td>\n",
              "      <td>3.1821</td>\n",
              "      <td>14.0137</td>\n",
              "      <td>0.5745</td>\n",
              "      <td>8.7989</td>\n",
              "      <td>14.5691</td>\n",
              "      <td>5.7487</td>\n",
              "      <td>-7.2393</td>\n",
              "      <td>4.2840</td>\n",
              "      <td>30.7133</td>\n",
              "      <td>10.5350</td>\n",
              "      <td>16.2191</td>\n",
              "      <td>2.5791</td>\n",
              "      <td>2.4716</td>\n",
              "      <td>14.3831</td>\n",
              "      <td>13.4325</td>\n",
              "      <td>-5.1488</td>\n",
              "      <td>-0.4073</td>\n",
              "      <td>4.9306</td>\n",
              "      <td>5.9965</td>\n",
              "      <td>-0.3085</td>\n",
              "      <td>12.9041</td>\n",
              "      <td>-3.8766</td>\n",
              "      <td>16.8911</td>\n",
              "      <td>11.1920</td>\n",
              "      <td>10.5785</td>\n",
              "      <td>0.6764</td>\n",
              "      <td>7.8871</td>\n",
              "      <td>...</td>\n",
              "      <td>15.4576</td>\n",
              "      <td>5.3133</td>\n",
              "      <td>3.6159</td>\n",
              "      <td>5.0384</td>\n",
              "      <td>6.6760</td>\n",
              "      <td>12.6644</td>\n",
              "      <td>2.7004</td>\n",
              "      <td>-0.6975</td>\n",
              "      <td>9.5981</td>\n",
              "      <td>5.4879</td>\n",
              "      <td>-4.7645</td>\n",
              "      <td>-8.4254</td>\n",
              "      <td>20.8773</td>\n",
              "      <td>3.1531</td>\n",
              "      <td>18.5618</td>\n",
              "      <td>7.7423</td>\n",
              "      <td>-10.1245</td>\n",
              "      <td>13.7241</td>\n",
              "      <td>-3.5189</td>\n",
              "      <td>1.7202</td>\n",
              "      <td>-8.4051</td>\n",
              "      <td>9.0164</td>\n",
              "      <td>3.0657</td>\n",
              "      <td>14.3691</td>\n",
              "      <td>25.8398</td>\n",
              "      <td>5.8764</td>\n",
              "      <td>11.8411</td>\n",
              "      <td>-19.7159</td>\n",
              "      <td>17.5743</td>\n",
              "      <td>0.5857</td>\n",
              "      <td>4.4354</td>\n",
              "      <td>3.9642</td>\n",
              "      <td>3.1364</td>\n",
              "      <td>1.6910</td>\n",
              "      <td>18.5227</td>\n",
              "      <td>-2.3978</td>\n",
              "      <td>7.8784</td>\n",
              "      <td>8.5635</td>\n",
              "      <td>12.7803</td>\n",
              "      <td>-1.0914</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>train_1</td>\n",
              "      <td>0</td>\n",
              "      <td>11.5006</td>\n",
              "      <td>-4.1473</td>\n",
              "      <td>13.8588</td>\n",
              "      <td>5.3890</td>\n",
              "      <td>12.3622</td>\n",
              "      <td>7.0433</td>\n",
              "      <td>5.6208</td>\n",
              "      <td>16.5338</td>\n",
              "      <td>3.1468</td>\n",
              "      <td>8.0851</td>\n",
              "      <td>-0.4032</td>\n",
              "      <td>8.0585</td>\n",
              "      <td>14.0239</td>\n",
              "      <td>8.4135</td>\n",
              "      <td>5.4345</td>\n",
              "      <td>13.7003</td>\n",
              "      <td>13.8275</td>\n",
              "      <td>-15.5849</td>\n",
              "      <td>7.8000</td>\n",
              "      <td>28.5708</td>\n",
              "      <td>3.4287</td>\n",
              "      <td>2.7407</td>\n",
              "      <td>8.5524</td>\n",
              "      <td>3.3716</td>\n",
              "      <td>6.9779</td>\n",
              "      <td>13.8910</td>\n",
              "      <td>-11.7684</td>\n",
              "      <td>-2.5586</td>\n",
              "      <td>5.0464</td>\n",
              "      <td>0.5481</td>\n",
              "      <td>-9.2987</td>\n",
              "      <td>7.8755</td>\n",
              "      <td>1.2859</td>\n",
              "      <td>19.3710</td>\n",
              "      <td>11.3702</td>\n",
              "      <td>0.7399</td>\n",
              "      <td>2.7995</td>\n",
              "      <td>5.8434</td>\n",
              "      <td>...</td>\n",
              "      <td>29.4846</td>\n",
              "      <td>5.8683</td>\n",
              "      <td>3.8208</td>\n",
              "      <td>15.8348</td>\n",
              "      <td>-5.0121</td>\n",
              "      <td>15.1345</td>\n",
              "      <td>3.2003</td>\n",
              "      <td>9.3192</td>\n",
              "      <td>3.8821</td>\n",
              "      <td>5.7999</td>\n",
              "      <td>5.5378</td>\n",
              "      <td>5.0988</td>\n",
              "      <td>22.0330</td>\n",
              "      <td>5.5134</td>\n",
              "      <td>30.2645</td>\n",
              "      <td>10.4968</td>\n",
              "      <td>-7.2352</td>\n",
              "      <td>16.5721</td>\n",
              "      <td>-7.3477</td>\n",
              "      <td>11.0752</td>\n",
              "      <td>-5.5937</td>\n",
              "      <td>9.4878</td>\n",
              "      <td>-14.9100</td>\n",
              "      <td>9.4245</td>\n",
              "      <td>22.5441</td>\n",
              "      <td>-4.8622</td>\n",
              "      <td>7.6543</td>\n",
              "      <td>-15.9319</td>\n",
              "      <td>13.3175</td>\n",
              "      <td>-0.3566</td>\n",
              "      <td>7.6421</td>\n",
              "      <td>7.7214</td>\n",
              "      <td>2.5837</td>\n",
              "      <td>10.9516</td>\n",
              "      <td>15.4305</td>\n",
              "      <td>2.0339</td>\n",
              "      <td>8.1267</td>\n",
              "      <td>8.7889</td>\n",
              "      <td>18.3560</td>\n",
              "      <td>1.9518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>train_2</td>\n",
              "      <td>0</td>\n",
              "      <td>8.6093</td>\n",
              "      <td>-2.7457</td>\n",
              "      <td>12.0805</td>\n",
              "      <td>7.8928</td>\n",
              "      <td>10.5825</td>\n",
              "      <td>-9.0837</td>\n",
              "      <td>6.9427</td>\n",
              "      <td>14.6155</td>\n",
              "      <td>-4.9193</td>\n",
              "      <td>5.9525</td>\n",
              "      <td>-0.3249</td>\n",
              "      <td>-11.2648</td>\n",
              "      <td>14.1929</td>\n",
              "      <td>7.3124</td>\n",
              "      <td>7.5244</td>\n",
              "      <td>14.6472</td>\n",
              "      <td>7.6782</td>\n",
              "      <td>-1.7395</td>\n",
              "      <td>4.7011</td>\n",
              "      <td>20.4775</td>\n",
              "      <td>17.7559</td>\n",
              "      <td>18.1377</td>\n",
              "      <td>1.2145</td>\n",
              "      <td>3.5137</td>\n",
              "      <td>5.6777</td>\n",
              "      <td>13.2177</td>\n",
              "      <td>-7.9940</td>\n",
              "      <td>-2.9029</td>\n",
              "      <td>5.8463</td>\n",
              "      <td>6.1439</td>\n",
              "      <td>-11.1025</td>\n",
              "      <td>12.4858</td>\n",
              "      <td>-2.2871</td>\n",
              "      <td>19.0422</td>\n",
              "      <td>11.0449</td>\n",
              "      <td>4.1087</td>\n",
              "      <td>4.6974</td>\n",
              "      <td>6.9346</td>\n",
              "      <td>...</td>\n",
              "      <td>13.2070</td>\n",
              "      <td>5.8442</td>\n",
              "      <td>4.7086</td>\n",
              "      <td>5.7141</td>\n",
              "      <td>-1.0410</td>\n",
              "      <td>20.5092</td>\n",
              "      <td>3.2790</td>\n",
              "      <td>-5.5952</td>\n",
              "      <td>7.3176</td>\n",
              "      <td>5.7690</td>\n",
              "      <td>-7.0927</td>\n",
              "      <td>-3.9116</td>\n",
              "      <td>7.2569</td>\n",
              "      <td>-5.8234</td>\n",
              "      <td>25.6820</td>\n",
              "      <td>10.9202</td>\n",
              "      <td>-0.3104</td>\n",
              "      <td>8.8438</td>\n",
              "      <td>-9.7009</td>\n",
              "      <td>2.4013</td>\n",
              "      <td>-4.2935</td>\n",
              "      <td>9.3908</td>\n",
              "      <td>-13.2648</td>\n",
              "      <td>3.1545</td>\n",
              "      <td>23.0866</td>\n",
              "      <td>-5.3000</td>\n",
              "      <td>5.3745</td>\n",
              "      <td>-6.2660</td>\n",
              "      <td>10.1934</td>\n",
              "      <td>-0.8417</td>\n",
              "      <td>2.9057</td>\n",
              "      <td>9.7905</td>\n",
              "      <td>1.6704</td>\n",
              "      <td>1.6858</td>\n",
              "      <td>21.6042</td>\n",
              "      <td>3.1417</td>\n",
              "      <td>-6.5213</td>\n",
              "      <td>8.2675</td>\n",
              "      <td>14.7222</td>\n",
              "      <td>0.3965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>train_3</td>\n",
              "      <td>0</td>\n",
              "      <td>11.0604</td>\n",
              "      <td>-2.1518</td>\n",
              "      <td>8.9522</td>\n",
              "      <td>7.1957</td>\n",
              "      <td>12.5846</td>\n",
              "      <td>-1.8361</td>\n",
              "      <td>5.8428</td>\n",
              "      <td>14.9250</td>\n",
              "      <td>-5.8609</td>\n",
              "      <td>8.2450</td>\n",
              "      <td>2.3061</td>\n",
              "      <td>2.8102</td>\n",
              "      <td>13.8463</td>\n",
              "      <td>11.9704</td>\n",
              "      <td>6.4569</td>\n",
              "      <td>14.8372</td>\n",
              "      <td>10.7430</td>\n",
              "      <td>-0.4299</td>\n",
              "      <td>15.9426</td>\n",
              "      <td>13.7257</td>\n",
              "      <td>20.3010</td>\n",
              "      <td>12.5579</td>\n",
              "      <td>6.8202</td>\n",
              "      <td>2.7229</td>\n",
              "      <td>12.1354</td>\n",
              "      <td>13.7367</td>\n",
              "      <td>0.8135</td>\n",
              "      <td>-0.9059</td>\n",
              "      <td>5.9070</td>\n",
              "      <td>2.8407</td>\n",
              "      <td>-15.2398</td>\n",
              "      <td>10.4407</td>\n",
              "      <td>-2.5731</td>\n",
              "      <td>6.1796</td>\n",
              "      <td>10.6093</td>\n",
              "      <td>-5.9158</td>\n",
              "      <td>8.1723</td>\n",
              "      <td>2.8521</td>\n",
              "      <td>...</td>\n",
              "      <td>31.8833</td>\n",
              "      <td>5.9684</td>\n",
              "      <td>7.2084</td>\n",
              "      <td>3.8899</td>\n",
              "      <td>-11.0882</td>\n",
              "      <td>17.2502</td>\n",
              "      <td>2.5881</td>\n",
              "      <td>-2.7018</td>\n",
              "      <td>0.5641</td>\n",
              "      <td>5.3430</td>\n",
              "      <td>-7.1541</td>\n",
              "      <td>-6.1920</td>\n",
              "      <td>18.2366</td>\n",
              "      <td>11.7134</td>\n",
              "      <td>14.7483</td>\n",
              "      <td>8.1013</td>\n",
              "      <td>11.8771</td>\n",
              "      <td>13.9552</td>\n",
              "      <td>-10.4701</td>\n",
              "      <td>5.6961</td>\n",
              "      <td>-3.7546</td>\n",
              "      <td>8.4117</td>\n",
              "      <td>1.8986</td>\n",
              "      <td>7.2601</td>\n",
              "      <td>-0.4639</td>\n",
              "      <td>-0.0498</td>\n",
              "      <td>7.9336</td>\n",
              "      <td>-12.8279</td>\n",
              "      <td>12.4124</td>\n",
              "      <td>1.8489</td>\n",
              "      <td>4.4666</td>\n",
              "      <td>4.7433</td>\n",
              "      <td>0.7178</td>\n",
              "      <td>1.4214</td>\n",
              "      <td>23.0347</td>\n",
              "      <td>-1.2706</td>\n",
              "      <td>-2.9275</td>\n",
              "      <td>10.2922</td>\n",
              "      <td>17.9697</td>\n",
              "      <td>-8.9996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>train_4</td>\n",
              "      <td>0</td>\n",
              "      <td>9.8369</td>\n",
              "      <td>-1.4834</td>\n",
              "      <td>12.8746</td>\n",
              "      <td>6.6375</td>\n",
              "      <td>12.2772</td>\n",
              "      <td>2.4486</td>\n",
              "      <td>5.9405</td>\n",
              "      <td>19.2514</td>\n",
              "      <td>6.2654</td>\n",
              "      <td>7.6784</td>\n",
              "      <td>-9.4458</td>\n",
              "      <td>-12.1419</td>\n",
              "      <td>13.8481</td>\n",
              "      <td>7.8895</td>\n",
              "      <td>7.7894</td>\n",
              "      <td>15.0553</td>\n",
              "      <td>8.4871</td>\n",
              "      <td>-3.0680</td>\n",
              "      <td>6.5263</td>\n",
              "      <td>11.3152</td>\n",
              "      <td>21.4246</td>\n",
              "      <td>18.9608</td>\n",
              "      <td>10.1102</td>\n",
              "      <td>2.7142</td>\n",
              "      <td>14.2080</td>\n",
              "      <td>13.5433</td>\n",
              "      <td>3.1736</td>\n",
              "      <td>-3.3423</td>\n",
              "      <td>5.9015</td>\n",
              "      <td>7.9352</td>\n",
              "      <td>-3.1582</td>\n",
              "      <td>9.4668</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>19.3239</td>\n",
              "      <td>12.4057</td>\n",
              "      <td>0.6329</td>\n",
              "      <td>2.7922</td>\n",
              "      <td>5.8184</td>\n",
              "      <td>...</td>\n",
              "      <td>33.5107</td>\n",
              "      <td>5.6953</td>\n",
              "      <td>5.4663</td>\n",
              "      <td>18.2201</td>\n",
              "      <td>6.5769</td>\n",
              "      <td>21.2607</td>\n",
              "      <td>3.2304</td>\n",
              "      <td>-1.7759</td>\n",
              "      <td>3.1283</td>\n",
              "      <td>5.5518</td>\n",
              "      <td>1.4493</td>\n",
              "      <td>-2.6627</td>\n",
              "      <td>19.8056</td>\n",
              "      <td>2.3705</td>\n",
              "      <td>18.4685</td>\n",
              "      <td>16.3309</td>\n",
              "      <td>-3.3456</td>\n",
              "      <td>13.5261</td>\n",
              "      <td>1.7189</td>\n",
              "      <td>5.1743</td>\n",
              "      <td>-7.6938</td>\n",
              "      <td>9.7685</td>\n",
              "      <td>4.8910</td>\n",
              "      <td>12.2198</td>\n",
              "      <td>11.8503</td>\n",
              "      <td>-7.8931</td>\n",
              "      <td>6.4209</td>\n",
              "      <td>5.9270</td>\n",
              "      <td>16.0201</td>\n",
              "      <td>-0.2829</td>\n",
              "      <td>-1.4905</td>\n",
              "      <td>9.5214</td>\n",
              "      <td>-0.1508</td>\n",
              "      <td>9.1942</td>\n",
              "      <td>13.2876</td>\n",
              "      <td>-1.5121</td>\n",
              "      <td>3.9267</td>\n",
              "      <td>9.5031</td>\n",
              "      <td>17.9974</td>\n",
              "      <td>-8.8104</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 202 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5dc105b-b475-4383-97ab-77c26ebcac65')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d5dc105b-b475-4383-97ab-77c26ebcac65 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d5dc105b-b475-4383-97ab-77c26ebcac65');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "print(train_df.shape)\n",
        "\n",
        "lr_clf = LogisticRegression()\n",
        "y_train = np.array(train_df['target'])\n",
        "print(train_df.columns)\n",
        "X_train = train_df.drop('target', axis=1).drop('ID_code', axis=1)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3)\n",
        "\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "\n",
        "# pca = PCA()\n",
        "# pca.fit(X_train)\n",
        "# X_train_pca = pca.transform(X_train)\n",
        "# X_val_pca = pca.transform(X_val)\n",
        "\n",
        "lr_clf.fit(X_train, y_train)\n",
        "\n",
        "print(roc_auc_score(y_val, lr_clf.predict_proba(X_val)[:, 1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWLFGSdSTjtQ",
        "outputId": "b868a57e-cabc-4682-f427-d4141a655aa0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(200000, 202)\n",
            "Index(['ID_code', 'target', 'var_0', 'var_1', 'var_2', 'var_3', 'var_4',\n",
            "       'var_5', 'var_6', 'var_7',\n",
            "       ...\n",
            "       'var_190', 'var_191', 'var_192', 'var_193', 'var_194', 'var_195',\n",
            "       'var_196', 'var_197', 'var_198', 'var_199'],\n",
            "      dtype='object', length=202)\n",
            "0.8590830509196538\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "dummy = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\n",
        "print(roc_auc_score(y_val, dummy.predict_proba(X_val)[:, 1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEydyjcSnml8",
        "outputId": "658b494e-7519-4e9d-bc59-b23c9ea58071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "train_data = lgb.Dataset(X_train, label=y_train)\n",
        "valid_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
        "\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'auc',                # –ú–æ–∂–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ: ['auc', 'binary_logloss']\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 31,                # –£–≤–µ–ª–∏—á–∏–≤–∞–π –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (–Ω–æ –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º)\n",
        "    'learning_rate': 0.05,           # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–Ω–æ–≥–æ, –º–æ–∂–Ω–æ 0.1, –µ—Å–ª–∏ –º–∞–ª–æ - –º–µ–Ω—å—à–µ\n",
        "    'bagging_freq': 5,                # –ß–∞—Å—Ç–æ—Ç–∞ –±—ç–≥–≥–∏–Ω–≥–∞\n",
        "    'verbose': -1,                    # –ü–æ–¥–∞–≤–ª—è–µ–º –≤—ã–≤–æ–¥ (—á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏–ª)\n",
        "    'num_threads': -1,                 # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å–µ —è–¥—Ä–∞\n",
        "    'random_state': 42,\n",
        "    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ (–ø–æ–º–æ–≥–∞—é—Ç –æ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è)\n",
        "    'lambda_l1': 0.1,\n",
        "    'lambda_l2': 0.1,\n",
        "    'min_child_samples': 20,           # –ú–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö –≤ –ª–∏—Å—Ç–µ\n",
        "}\n",
        "\n",
        "model = lgb.train(\n",
        "    params,\n",
        "    train_set=train_data,\n",
        "    valid_sets=[valid_data]\n",
        ")\n",
        "\n",
        "y_pred_prob = model.predict(X_val, num_iteration=model.best_iteration)\n",
        "print(y_pred_prob.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAT_f2gzc-OO",
        "outputId": "70d88a72-a2ac-438e-ad92-960c50cb624f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "print(roc_auc_score(y_val, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCX0p3q6fVlV",
        "outputId": "70e9fddc-e574-4eb5-b36b-5b4aee96178c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.508004834578461\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "input_size = 200\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(input_size, 300),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm1d(300),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(300, 600),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm1d(600),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(600, 64),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm1d(64),\n",
        "    nn.Dropout(0.2),\n",
        "    nn.Linear(64, 2),\n",
        "    nn.ReLU()\n",
        ")\n",
        "\n",
        "X_train_tensor = torch.FloatTensor(X_train)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "X_val_tensor = torch.FloatTensor(X_val)\n",
        "y_val_tensor = torch.LongTensor(y_val)\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ DataLoader\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for inputs, labels in loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return total_loss / len(loader), 100 * correct / total\n",
        "\n",
        "def validate_with_auc(model, loader, criterion, device):\n",
        "    \"\"\"\n",
        "    –í–∞–ª–∏–¥–∞—Ü–∏—è —Å —Ä–∞—Å—á–µ—Ç–æ–º ROC-AUC\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –º–µ—Ç–∫–∏\n",
        "    all_probs = []  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è positive –∫–ª–∞—Å—Å–∞\n",
        "    all_labels = [] # –ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ softmax\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "\n",
        "            # –ë–µ—Ä–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å positive –∫–ª–∞—Å—Å–∞ (–∏–Ω–¥–µ–∫—Å 1)\n",
        "            positive_probs = probs[:, 1].cpu().numpy()\n",
        "\n",
        "            # –°–æ—Ö—Ä–∞–Ω—è–µ–º\n",
        "            all_probs.extend(positive_probs)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            # –°—á–∏—Ç–∞–µ–º accuracy\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy –º–∞—Å—Å–∏–≤—ã\n",
        "    all_probs = np.array(all_probs)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    # –°—á–∏—Ç–∞–µ–º ROC-AUC\n",
        "    try:\n",
        "        auc_score = roc_auc_score(all_labels, all_probs)\n",
        "    except ValueError as e:\n",
        "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ AUC: {e}\")\n",
        "        auc_score = 0.5  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
        "\n",
        "    return {\n",
        "        'loss': total_loss / len(loader),\n",
        "        'accuracy': 100 * correct / total,\n",
        "        'auc': auc_score,\n",
        "        'probabilities': all_probs,\n",
        "        'true_labels': all_labels\n",
        "    }\n",
        "\n",
        "# –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏\n",
        "num_epochs = 50\n",
        "best_val_acc = 0\n",
        "best_auc = 0\n",
        "best_epoch = 0\n",
        "history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_auc': []}\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "    # –í–∞–ª–∏–¥–∞—Ü–∏—è\n",
        "    val_results = validate_with_auc(model, val_loader, criterion, device)\n",
        "\n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_results['loss'])\n",
        "    history['val_acc'].append(val_results['accuracy'])\n",
        "    history['val_auc'].append(val_results['auc'])\n",
        "\n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –ø–æ AUC\n",
        "    if val_results['auc'] > best_auc:\n",
        "        best_auc = val_results['auc']\n",
        "        best_epoch = epoch + 1\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'auc': best_auc,\n",
        "            'accuracy': val_results['accuracy']\n",
        "        }, 'best_model_auc.pth')\n",
        "        print(f\"‚úÖ –≠–ø–æ—Ö–∞ {epoch+1}: –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å! AUC = {best_auc:.4f}\")\n",
        "\n",
        "print(f'\\nüèÜ –õ—É—á—à–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {best_val_acc:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "rM5wcB8OpAN7",
        "outputId": "ea750fe1-024a-43fd-ef56-28d0a32eb736"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ –≠–ø–æ—Ö–∞ 1: –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å! AUC = 0.8431\n",
            "‚úÖ –≠–ø–æ—Ö–∞ 2: –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å! AUC = 0.8527\n",
            "‚úÖ –≠–ø–æ—Ö–∞ 3: –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å! AUC = 0.8555\n",
            "‚úÖ –≠–ø–æ—Ö–∞ 6: –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å! AUC = 0.8573\n",
            "‚úÖ –≠–ø–æ—Ö–∞ 7: –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å! AUC = 0.8573\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1632054186.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m# –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# –í–∞–ª–∏–¥–∞—Ü–∏—è\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1632054186.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mRuns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}