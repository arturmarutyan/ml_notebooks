{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOYj6bz9R9wcKDgKwru+DNN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vGRgxSR7aCD3","executionInfo":{"status":"ok","timestamp":1771397697664,"user_tz":-180,"elapsed":1600511,"user":{"displayName":"Артур Марутян","userId":"16947372880826863666"}},"outputId":"ab8bf64b-db83-4655-e095-bb2286306449"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","==================================================\n","BiLSTM for Part-of-Speech Tagging\n","==================================================\n","Loading UDPOS dataset...\n","Training examples: 12544\n","Validation examples: 2001\n","Test examples: 2077\n","\n","Building vocabularies...\n","Word vocabulary size: 10099\n","Tag vocabulary size: 20\n","Top 10 most common words: [('.', 8640), ('the', 8151), (',', 7021), ('to', 5076), ('and', 4855), ('a', 3609), ('of', 3589), ('I', 3123), ('in', 2911), ('is', 2154)]\n","Tags: ['<pad>', '<unk>', 'NOUN', 'PUNCT', 'VERB', 'PRON', 'ADP', 'DET', 'ADJ', 'AUX', 'PROPN', 'ADV', 'CCONJ', 'PART', 'NUM', 'SCONJ', '_', 'SYM', 'INTJ', 'X']\n","\n","Initializing model...\n","Model has 1,645,824 trainable parameters\n","\n","Starting training...\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [01:42<00:00,  1.05s/it]\n","Evaluating: 100%|██████████| 16/16 [00:03<00:00,  4.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  -> New best model saved!\n","Epoch: 01 | Time: 1m 46s\n","  Train Loss: 1.764 | Train Acc: 45.47%\n","  Val Loss: 0.765 | Val Acc: 76.41%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [01:42<00:00,  1.04s/it]\n","Evaluating: 100%|██████████| 16/16 [00:04<00:00,  3.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  -> New best model saved!\n","Epoch: 02 | Time: 1m 46s\n","  Train Loss: 0.549 | Train Acc: 83.34%\n","  Val Loss: 0.382 | Val Acc: 88.27%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [01:40<00:00,  1.03s/it]\n","Evaluating: 100%|██████████| 16/16 [00:03<00:00,  4.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  -> New best model saved!\n","Epoch: 03 | Time: 1m 43s\n","  Train Loss: 0.306 | Train Acc: 90.86%\n","  Val Loss: 0.318 | Val Acc: 90.11%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [01:40<00:00,  1.03s/it]\n","Evaluating: 100%|██████████| 16/16 [00:03<00:00,  4.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  -> New best model saved!\n","Epoch: 04 | Time: 1m 44s\n","  Train Loss: 0.226 | Train Acc: 93.22%\n","  Val Loss: 0.291 | Val Acc: 91.06%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [01:43<00:00,  1.06s/it]\n","Evaluating: 100%|██████████| 16/16 [00:03<00:00,  4.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  -> New best model saved!\n","Epoch: 05 | Time: 1m 47s\n","  Train Loss: 0.186 | Train Acc: 94.32%\n","  Val Loss: 0.275 | Val Acc: 91.62%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [01:40<00:00,  1.03s/it]\n","Evaluating: 100%|██████████| 16/16 [00:03<00:00,  4.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  -> New best model saved!\n","Epoch: 06 | Time: 1m 44s\n","  Train Loss: 0.164 | Train Acc: 94.95%\n","  Val Loss: 0.272 | Val Acc: 91.58%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [01:40<00:00,  1.02s/it]\n","Evaluating: 100%|██████████| 16/16 [00:04<00:00,  3.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  -> New best model saved!\n","Epoch: 07 | Time: 1m 44s\n","  Train Loss: 0.146 | Train Acc: 95.41%\n","  Val Loss: 0.267 | Val Acc: 91.91%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [01:40<00:00,  1.02s/it]\n","Evaluating: 100%|██████████| 16/16 [00:03<00:00,  4.62it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 08 | Time: 1m 43s\n","  Train Loss: 0.133 | Train Acc: 95.80%\n","  Val Loss: 0.271 | Val Acc: 91.82%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [01:40<00:00,  1.03s/it]\n","Evaluating: 100%|██████████| 16/16 [00:04<00:00,  3.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 09 | Time: 1m 44s\n","  Train Loss: 0.121 | Train Acc: 96.16%\n","  Val Loss: 0.267 | Val Acc: 92.08%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [01:41<00:00,  1.03s/it]\n","Evaluating: 100%|██████████| 16/16 [00:03<00:00,  4.54it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 10 | Time: 1m 44s\n","  Train Loss: 0.109 | Train Acc: 96.54%\n","  Val Loss: 0.273 | Val Acc: 91.86%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [01:40<00:00,  1.02s/it]\n","Evaluating: 100%|██████████| 16/16 [00:03<00:00,  4.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 11 | Time: 1m 43s\n","  Train Loss: 0.100 | Train Acc: 96.84%\n","  Val Loss: 0.274 | Val Acc: 92.01%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [01:42<00:00,  1.05s/it]\n","Evaluating: 100%|██████████| 16/16 [00:03<00:00,  4.64it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 12 | Time: 1m 45s\n","  Train Loss: 0.092 | Train Acc: 97.06%\n","  Val Loss: 0.278 | Val Acc: 92.27%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [01:42<00:00,  1.04s/it]\n","Evaluating: 100%|██████████| 16/16 [00:03<00:00,  4.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 13 | Time: 1m 45s\n","  Train Loss: 0.085 | Train Acc: 97.31%\n","  Val Loss: 0.286 | Val Acc: 92.05%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [01:40<00:00,  1.03s/it]\n","Evaluating: 100%|██████████| 16/16 [00:03<00:00,  4.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 14 | Time: 1m 44s\n","  Train Loss: 0.078 | Train Acc: 97.53%\n","  Val Loss: 0.296 | Val Acc: 92.07%\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 98/98 [01:41<00:00,  1.03s/it]\n","Evaluating: 100%|██████████| 16/16 [00:03<00:00,  4.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 15 | Time: 1m 44s\n","  Train Loss: 0.071 | Train Acc: 97.72%\n","  Val Loss: 0.299 | Val Acc: 92.10%\n","\n","==================================================\n","Testing best model...\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 17/17 [00:04<00:00,  4.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test Loss: 0.254 | Test Acc: 91.91%\n","\n","==================================================\n","Inference example:\n","\n","Sentence: The Queen will deliver a speech about the conflict in North Korea at 1pm tomorrow.\n","Unknown tokens: ['korea']\n","\n","Predictions:\n","  DET        → the\n","  NOUN       → queen\n","  AUX        → will\n","  VERB       → deliver\n","  DET        → a\n","  NOUN       → speech\n","  ADP        → about\n","  DET        → the\n","  NOUN       → conflict\n","  ADP        → in\n","  PROPN      → north\n","  PROPN      → korea\n","  ADP        → at\n","  NUM        → 1\n","  NOUN       → pm\n","  NOUN       → tomorrow\n","  PUNCT      → .\n","\n","==================================================\n","Training completed successfully!\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","\n","import spacy\n","import numpy as np\n","from collections import Counter\n","from tqdm import tqdm\n","import time\n","import random\n","import os\n","import requests\n","import zipfile\n","from typing import List, Dict, Tuple, Optional\n","\n","SEED = 1234\n","BATCH_SIZE = 128\n","EMBEDDING_DIM = 100\n","HIDDEN_DIM = 128\n","N_LAYERS = 2\n","DROPOUT = 0.25\n","N_EPOCHS = 15\n","MIN_FREQ = 2\n","MAX_SENT_LEN = 100\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","def download_udpos_data():\n","    \"\"\"Download UDPOS dataset from GitHub\"\"\"\n","    url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/\"\n","    files = {\n","        'train': 'en_ewt-ud-train.conllu',\n","        'valid': 'en_ewt-ud-dev.conllu',\n","        'test': 'en_ewt-ud-test.conllu'\n","    }\n","\n","    data = {}\n","    for split, filename in files.items():\n","        response = requests.get(url + filename)\n","        data[split] = response.text\n","        with open(filename, 'w', encoding='utf-8') as f:\n","            f.write(response.text)\n","\n","    return data\n","\n","def parse_conllu(content: str) -> List[Dict]:\n","    \"\"\"Parse CONLL-U format data\"\"\"\n","    sentences = []\n","    current_sent = {'tokens': [], 'upos': [], 'xpos': []}\n","\n","    for line in content.split('\\n'):\n","        line = line.strip()\n","        if not line:\n","            if current_sent['tokens']:\n","                sentences.append(current_sent)\n","                current_sent = {'tokens': [], 'upos': [], 'xpos': []}\n","        elif not line.startswith('#'):\n","            parts = line.split('\\t')\n","            if len(parts) >= 4:\n","                current_sent['tokens'].append(parts[1])  # word form\n","                current_sent['upos'].append(parts[3])    # universal POS\n","                if len(parts) > 4:\n","                    current_sent['xpos'].append(parts[4])  # language-specific POS\n","\n","    return sentences\n","\n","def load_udpos_data() -> Tuple[List, List, List]:\n","    print(\"Loading UDPOS dataset...\")\n","\n","    # Try to download if not exists\n","    if not all(os.path.exists(f) for f in ['en_ewt-ud-train.conllu',\n","                                            'en_ewt-ud-dev.conllu',\n","                                            'en_ewt-ud-test.conllu']):\n","        download_udpos_data()\n","\n","    # Parse data\n","    with open('en_ewt-ud-train.conllu', 'r', encoding='utf-8') as f:\n","        train_data = parse_conllu(f.read())\n","    with open('en_ewt-ud-dev.conllu', 'r', encoding='utf-8') as f:\n","        valid_data = parse_conllu(f.read())\n","    with open('en_ewt-ud-test.conllu', 'r', encoding='utf-8') as f:\n","        test_data = parse_conllu(f.read())\n","\n","    print(f\"Training examples: {len(train_data)}\")\n","    print(f\"Validation examples: {len(valid_data)}\")\n","    print(f\"Test examples: {len(test_data)}\")\n","\n","    return train_data, valid_data, test_data\n","\n","class Vocabulary:\n","    def __init__(self, min_freq: int = 1, specials: List[str] = ['<pad>', '<unk>']):\n","        self.min_freq = min_freq\n","        self.specials = specials\n","        self.itos: List[str] = []\n","        self.stoi: Dict[str, int] = {}\n","        self.freqs: Counter = Counter()\n","\n","    def build_vocab(self, sentences: List[List[str]]):\n","        for sent in sentences:\n","            self.freqs.update(sent)\n","\n","        for special in self.specials:\n","            self.itos.append(special)\n","            self.stoi[special] = len(self.itos) - 1\n","\n","        for word, freq in self.freqs.most_common():\n","            if freq >= self.min_freq and word not in self.stoi:\n","                self.itos.append(word)\n","                self.stoi[word] = len(self.itos) - 1\n","\n","    def numericalize(self, tokens: List[str], max_len: Optional[int] = None) -> List[int]:\n","        if max_len:\n","            tokens = tokens[:max_len]\n","        unk_idx = self.stoi['<unk>']\n","        return [self.stoi.get(token, unk_idx) for token in tokens]\n","\n","    def __len__(self) -> int:\n","        return len(self.itos)\n","\n","# ==================== DATASET ====================\n","class PoSDataset(Dataset):\n","    def __init__(self, data: List[Dict], word_vocab: Vocabulary, tag_vocab: Vocabulary, max_len: int = MAX_SENT_LEN):\n","        self.data = data\n","        self.word_vocab = word_vocab\n","        self.tag_vocab = tag_vocab\n","        self.max_len = max_len\n","\n","    def __len__(self) -> int:\n","        return len(self.data)\n","\n","    def __getitem__(self, idx: int) -> Dict:\n","        item = self.data[idx]\n","\n","        tokens = item['tokens'][:self.max_len]\n","        tags = item['upos'][:self.max_len]\n","\n","        token_ids = self.word_vocab.numericalize(tokens)\n","        tag_ids = self.tag_vocab.numericalize(tags)\n","\n","        return {\n","            'tokens': tokens,\n","            'token_ids': torch.tensor(token_ids),\n","            'tag_ids': torch.tensor(tag_ids),\n","            'length': len(token_ids)\n","        }\n","\n","def collate_fn(batch: List[Dict]) -> Dict:\n","    max_len = max([item['length'] for item in batch])\n","    pad_idx = word_vocab.stoi['<pad>']\n","    tag_pad_idx = tag_vocab.stoi['<pad>']\n","\n","    token_tensor = torch.full((max_len, len(batch)), pad_idx, dtype=torch.long)\n","    tag_tensor = torch.full((max_len, len(batch)), tag_pad_idx, dtype=torch.long)\n","\n","    for i, item in enumerate(batch):\n","        length = item['length']\n","        token_tensor[:length, i] = item['token_ids']\n","        tag_tensor[:length, i] = item['tag_ids']\n","\n","    return {\n","        'text': token_tensor.to(device),\n","        'tags': tag_tensor.to(device),\n","        'lengths': torch.tensor([item['length'] for item in batch])\n","    }\n","\n","class BiLSTMPOSTagger(nn.Module):\n","    def __init__(self,\n","                 input_dim: int,\n","                 embedding_dim: int,\n","                 hidden_dim: int,\n","                 output_dim: int,\n","                 n_layers: int,\n","                 bidirectional: bool,\n","                 dropout: float,\n","                 pad_idx: int):\n","\n","        super().__init__()\n","\n","        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n","\n","        self.lstm = nn.LSTM(embedding_dim,\n","                            hidden_dim,\n","                            num_layers=n_layers,\n","                            bidirectional=bidirectional,\n","                            dropout=dropout if n_layers > 1 else 0,\n","                            batch_first=False)\n","\n","        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, text: torch.Tensor) -> torch.Tensor:\n","        # text = [seq_len, batch_size]\n","\n","        embedded = self.dropout(self.embedding(text))\n","        # embedded = [seq_len, batch_size, emb_dim]\n","\n","        outputs, (hidden, cell) = self.lstm(embedded)\n","        # outputs = [seq_len, batch_size, hid_dim * n_directions]\n","\n","        predictions = self.fc(self.dropout(outputs))\n","        # predictions = [seq_len, batch_size, output_dim]\n","\n","        return predictions\n","\n","def init_weights(m: nn.Module):\n","    for name, param in m.named_parameters():\n","        if 'weight' in name:\n","            nn.init.normal_(param.data, mean=0, std=0.1)\n","        else:\n","            nn.init.constant_(param.data, 0)\n","\n","def count_parameters(model: nn.Module) -> int:\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","def categorical_accuracy(preds: torch.Tensor, y: torch.Tensor, tag_pad_idx: int) -> torch.Tensor:\n","    max_preds = preds.argmax(dim=1, keepdim=True)\n","    non_pad_elements = (y != tag_pad_idx).nonzero(as_tuple=True)[0]\n","    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n","    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]]).to(device)\n","\n","def epoch_time(start_time: float, end_time: float) -> Tuple[int, int]:\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs\n","\n","def train(model: nn.Module,\n","          dataloader: DataLoader,\n","          optimizer: optim.Optimizer,\n","          criterion: nn.Module,\n","          tag_pad_idx: int) -> Tuple[float, float]:\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    model.train()\n","\n","    for batch in tqdm(dataloader, desc='Training'):\n","        text = batch['text']\n","        tags = batch['tags']\n","\n","        optimizer.zero_grad()\n","\n","        predictions = model(text)\n","\n","        predictions = predictions.view(-1, predictions.shape[-1])\n","        tags = tags.view(-1)\n","\n","        loss = criterion(predictions, tags)\n","        acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","        epoch_acc += acc.item()\n","\n","    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)\n","\n","def evaluate(model: nn.Module,\n","             dataloader: DataLoader,\n","             criterion: nn.Module,\n","             tag_pad_idx: int) -> Tuple[float, float]:\n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader, desc='Evaluating'):\n","            text = batch['text']\n","            tags = batch['tags']\n","\n","            predictions = model(text)\n","\n","            predictions = predictions.view(-1, predictions.shape[-1])\n","            tags = tags.view(-1)\n","\n","            loss = criterion(predictions, tags)\n","            acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n","\n","            epoch_loss += loss.item()\n","            epoch_acc += acc.item()\n","\n","    return epoch_loss / len(dataloader), epoch_acc / len(dataloader)\n","\n","def tag_sentence(model: nn.Module,\n","                 sentence: str,\n","                 word_vocab: Vocabulary,\n","                 tag_vocab: Vocabulary,\n","                 device: torch.device) -> Tuple[List[str], List[str], List[str]]:\n","    model.eval()\n","\n","    try:\n","        nlp = spacy.load('en_core_web_sm')\n","    except:\n","        os.system('python -m spacy download en_core_web_sm')\n","        nlp = spacy.load('en_core_web_sm')\n","\n","    tokens = [token.text.lower() for token in nlp(sentence)]\n","\n","    unk_idx = word_vocab.stoi['<unk>']\n","    token_ids = word_vocab.numericalize(tokens)\n","    unks = [t for t, idx in zip(tokens, token_ids) if idx == unk_idx]\n","\n","    token_tensor = torch.tensor(token_ids).unsqueeze(-1).to(device)\n","\n","    with torch.no_grad():\n","        predictions = model(token_tensor)\n","        predicted_indices = predictions.argmax(-1).squeeze().cpu().numpy()\n","\n","    predicted_tags = [tag_vocab.itos[idx] for idx in predicted_indices]\n","\n","    return tokens, predicted_tags, unks\n","\n","if __name__ == \"__main__\":\n","    print(\"=\" * 50)\n","    print(\"BiLSTM for Part-of-Speech Tagging\")\n","    print(\"=\" * 50)\n","\n","    # 1. Load data\n","    train_data_raw, valid_data_raw, test_data_raw = load_udpos_data()\n","\n","    # 2. Build vocabularies\n","    print(\"\\nBuilding vocabularies...\")\n","\n","    # Word vocabulary\n","    word_vocab = Vocabulary(min_freq=MIN_FREQ)\n","    word_vocab.build_vocab([sent['tokens'] for sent in train_data_raw])\n","\n","    # Tag vocabulary (UD tags)\n","    tag_vocab = Vocabulary(min_freq=1)  # Include all tags\n","    tag_vocab.build_vocab([[tag] for sent in train_data_raw for tag in sent['upos']])\n","\n","    print(f\"Word vocabulary size: {len(word_vocab)}\")\n","    print(f\"Tag vocabulary size: {len(tag_vocab)}\")\n","    print(f\"Top 10 most common words: {word_vocab.freqs.most_common(10)}\")\n","    print(f\"Tags: {tag_vocab.itos}\")\n","\n","    # 3. Create datasets\n","    train_dataset = PoSDataset(train_data_raw, word_vocab, tag_vocab)\n","    valid_dataset = PoSDataset(valid_data_raw, word_vocab, tag_vocab)\n","    test_dataset = PoSDataset(test_data_raw, word_vocab, tag_vocab)\n","\n","    # 4. Create data loaders\n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n","                             shuffle=True, collate_fn=collate_fn)\n","    valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE,\n","                             shuffle=False, collate_fn=collate_fn)\n","    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n","                            shuffle=False, collate_fn=collate_fn)\n","\n","    # 5. Initialize model\n","    print(\"\\nInitializing model...\")\n","    INPUT_DIM = len(word_vocab)\n","    OUTPUT_DIM = len(tag_vocab)\n","    PAD_IDX = word_vocab.stoi['<pad>']\n","    TAG_PAD_IDX = tag_vocab.stoi['<pad>']\n","\n","    model = BiLSTMPOSTagger(INPUT_DIM,\n","                           EMBEDDING_DIM,\n","                           HIDDEN_DIM,\n","                           OUTPUT_DIM,\n","                           N_LAYERS,\n","                           True,  # bidirectional\n","                           DROPOUT,\n","                           PAD_IDX)\n","\n","    model.apply(init_weights)\n","    model = model.to(device)\n","\n","    print(f\"Model has {count_parameters(model):,} trainable parameters\")\n","\n","    # 6. Setup training\n","    optimizer = optim.Adam(model.parameters())\n","    criterion = nn.CrossEntropyLoss(ignore_index=TAG_PAD_IDX)\n","\n","    # 7. Traininhttps://raw.githubusercontent.comg loop\n","    print(\"\\nStarting training...\")\n","    best_valid_loss = float('inf')\n","\n","    for epoch in range(N_EPOCHS):\n","        start_time = time.time()\n","\n","        train_loss, train_acc = train(model, train_loader, optimizer, criterion, TAG_PAD_IDX)\n","        valid_loss, valid_acc = evaluate(model, valid_loader, criterion, TAG_PAD_IDX)\n","\n","        end_time = time.time()\n","        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","        # Save best model\n","        if valid_loss < best_valid_loss:\n","            best_valid_loss = valid_loss\n","            torch.save(model.state_dict(), 'best_pos_model.pt')\n","            print(f\"  -> New best model saved!\")\n","\n","        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","        print(f'  Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","        print(f'  Val Loss: {valid_loss:.3f} | Val Acc: {valid_acc*100:.2f}%')\n","\n","    # 8. Test evaluation\n","    print(\"\\n\" + \"=\" * 50)\n","    print(\"Testing best model...\")\n","    model.load_state_dict(torch.load('best_pos_model.pt'))\n","    test_loss, test_acc = evaluate(model, test_loader, criterion, TAG_PAD_IDX)\n","    print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')\n","\n","    # 9. Inference example\n","    print(\"\\n\" + \"=\" * 50)\n","    print(\"Inference example:\")\n","    test_sentence = \"The Queen will deliver a speech about the conflict in North Korea at 1pm tomorrow.\"\n","    tokens, tags, unks = tag_sentence(model, test_sentence, word_vocab, tag_vocab, device)\n","\n","    print(f\"\\nSentence: {test_sentence}\")\n","    print(f\"Unknown tokens: {unks}\")\n","    print(\"\\nPredictions:\")\n","    for token, tag in zip(tokens, tags):\n","        print(f\"  {tag:10} → {token}\")\n","\n","    print(\"\\n\" + \"=\" * 50)\n","    print(\"Training completed successfully!\")"]}]}